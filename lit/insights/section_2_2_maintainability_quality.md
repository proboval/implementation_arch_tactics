## Topic: Maintainability & Quality Models (Section 2.2)

**Papers:** 5 + ISO 25010 | **Updated:** 2026-02-16

### Summary

Software quality models have evolved through four major generations, each progressively refining how maintainability is decomposed and measured. McCall's 1977 model first established quality factors (including maintainability) with a hierarchical factors-criteria-metrics structure, but its metrics were "neither clearly nor completely defined" (Al-Qutaish 2010). Boehm's 1978 model introduced a similar hierarchy with portability and utility as top-level characteristics, adding human judgment aspects but still lacking operationalization. The FURPS model (Grady 1992) simplified quality into five factors but scored lowest in factor coverage at 20.34% (Al-Badareen et al. 2011), omitting key concerns like portability. ISO 9126 (2001) consolidated international consensus, decomposing maintainability into four sub-characteristics: analyzability, changeability, stability, and testability. Its successor, ISO/IEC 25010 (2011), refined these into five sub-characteristics -- modularity, reusability, analysability, modifiability, and testability -- providing the most comprehensive and internationally agreed-upon framework for evaluating software maintainability.

The dominance of maintainability as a cost driver is well-established: maintenance activities account for 60-80% of the total software lifecycle cost, making it the single largest budget item in long-lived systems. Molnar and Motogna (2020) provide longitudinal empirical evidence from 111 releases of three open-source Java applications spanning over a decade, demonstrating that maintainability effort concentrates in a small subset of packages (e.g., 6 packages in jEdit account for approximately 80% of maintenance effort) and that mature application versions stabilize in quality, suggesting that early architectural investment pays compounding dividends. Their comparison of three quantitative models (Maintainability Index, ARiSA, SQALE) found that SQALE -- implemented in SonarQube as Technical Debt Ratio -- is the most reliable system-level maintainability measure, being independent of size-related confounds that plague MI and ARiSA at the system level.

Bridging the gap between abstract quality models and actionable practice, the SIG/Visser model (2016) provides 10 concrete guidelines mapped to measurable metrics with empirically calibrated thresholds derived from benchmarking hundreds of real systems. These guidelines -- including short units (<= 15 LOC), low cyclomatic complexity (<= 5), minimal duplication, small interfaces (<= 4 parameters), loose coupling, balanced components, small codebase, automated tests (>= 80% coverage), and clean code -- aggregate into ISO 25010 sub-characteristics (analysability, modifiability, testability, modularity, reusability) via star ratings (1-5). Critically, the SIG model demonstrates predictive power: issue resolution is 2x faster in 4-star systems compared to 2-star systems. Bass, Clements, and Kazman (2021) complete the picture by providing quality attribute scenarios -- a structured way to specify maintainability requirements -- and a taxonomy of 15 modifiability tactics (5 cohesion, 4 coupling, 6 binding-time) that serve as architectural building blocks for achieving quality goals. The Visser guidelines map cleanly onto Bass's tactic categories: short/simple units and separation of concerns correspond to "Increase Cohesion" tactics, loose coupling guidelines correspond to "Reduce Coupling" tactics, and the "Write Code Once" principle corresponds to "Abstract Common Functionality."

### Key Papers
| Paper | Contribution | Relevance |
|-------|--------------|-----------|
| `bass2021software` | Canonical taxonomy of modifiability tactics (15 tactics in 3 groups); quality attribute scenarios as formal specification method; distinction between tactics and patterns | CRITICAL |
| `visser2016maintainable` | 10 actionable guidelines with metric thresholds benchmarked against hundreds of systems; SIG star ratings mapped to ISO 25010 sub-characteristics; 2x issue resolution speed in high-rated systems | HIGH |
| `molnar2020study` | Longitudinal empirical comparison of MI, ARiSA, SQALE across 111 releases; SQALE as best system-level model; maintainability independence from size; effort concentration in hotspot packages | HIGH |
| `alqutaish2010quality` | Analytical comparison of 5 quality models (McCall, Boehm, Dromey, FURPS, ISO 9126); traces evolution of maintainability decomposition; recommends ISO 9126 as most useful model | MEDIUM |
| `albadareen2011quality` | Mathematical weight-based comparison of quality models; quantifies factor coverage gaps (McCall 63.67%, FURPS 20.34%); shows ISO framework as most balanced | MEDIUM |
| ISO/IEC 25010 (2011) | International standard defining maintainability as modularity + reusability + analysability + modifiability + testability; successor to ISO 9126 | CRITICAL |

### Consensus
| Finding | Papers | Confidence |
|---------|--------|------------|
| ISO-family models (9126/25010) provide the most comprehensive and balanced quality framework | `alqutaish2010quality`, `albadareen2011quality`, `visser2016maintainable`, `bass2021software` | High |
| Maintainability is decomposable into measurable sub-characteristics | All 5 papers + ISO 25010 | High |
| Earlier quality models (McCall, Boehm, FURPS) have significant coverage gaps and poorly defined metrics | `alqutaish2010quality`, `albadareen2011quality` | High |
| SQALE/Technical Debt Ratio is the most reliable system-level maintainability measure | `molnar2020study`, `visser2016maintainable` (SIG model uses similar approach) | High |
| Maintainability effort concentrates in hotspot packages, not uniformly across the codebase | `molnar2020study` | Medium (single study, 3 applications) |
| Unit-level code quality aggregates upward to determine architecture-level maintainability | `visser2016maintainable`, `bass2021software` | High |
| Mature, well-architected systems stabilize in maintainability over time | `molnar2020study` | Medium |
| Reliability is the only quality factor common to all pre-ISO models | `alqutaish2010quality` | High |

### Contradictions
| Issue | Position A | Position B | Thesis Choice |
|-------|------------|------------|---------------|
| Best granularity for maintainability measurement | `molnar2020study`: MI useful at class/method level but unreliable at system level due to size confounding | `visser2016maintainable`: Unit-level metrics (which include MI-like measures) are fundamental and aggregate upward | Use both: SQALE/TDR for system-level before/after comparison, MI-derived metrics (complexity, LOC) at unit level for fine-grained analysis |
| Scope of maintainability definition | `alqutaish2010quality`: ISO 9126 decomposes into analyzability, changeability, stability, testability (4 sub-chars) | ISO 25010: Replaces with modularity, reusability, analysability, modifiability, testability (5 sub-chars) | Adopt ISO 25010 as the current standard; note the evolution from ISO 9126 |
| Role of code size in maintainability | `molnar2020study`: Maintainability is independent of system size (SQALE confirms this) | `visser2016maintainable`: Guideline 8 explicitly says "Keep Codebase Small" as maintainability factor | Size affects effort/cost but not necessarily quality ratios; SQALE normalizes by size. Acknowledge both perspectives: absolute effort grows with size but maintainability rate need not degrade |

### Gaps
| Gap | Impact on Thesis |
|-----|-----------------|
| No empirical study mapping ISO 25010 sub-characteristics directly to specific architectural tactic applications | Thesis must establish this mapping (O1) by connecting Bass's modifiability tactics to ISO 25010 sub-characteristics and measuring impact per sub-characteristic |
| Quality model comparisons (Al-Badareen, Al-Qutaish) are purely analytical with no empirical validation on real projects | Thesis provides empirical validation by measuring ISO 25010-aligned metrics before/after tactic implementation (O3, O4) |
| Molnar's study is limited to Java GUI applications; no longitudinal maintainability studies on Python backend systems | Thesis targets Python backend systems, extending the empirical evidence base for maintainability measurement to a new language/domain |
| SIG model benchmarks are proprietary and not publicly reproducible | Thesis uses open-source tools (Radon, SonarQube) with transparent thresholds; can reference SIG thresholds as guidelines but must define own benchmark criteria |
| No study combines quality model evaluation with LLM-driven architectural intervention | This is the core thesis contribution: using quality models to both guide and evaluate LLM-implemented tactic changes (O2, O3) |
| Visser's predictive finding (2x faster issue resolution in 4-star systems) lacks independent replication | Thesis can cite as motivation but should not rely on this specific claim; focus on measurable metric improvement instead |

### Recommendations
**Adopt:** ISO 25010's five maintainability sub-characteristics (modularity, reusability, analysability, modifiability, testability) as the evaluation framework for thesis Objectives O3 and O4. Use SQALE/Technical Debt Ratio via SonarQube as the primary system-level metric, following Molnar's empirical validation. Adopt Bass's quality attribute scenario format for specifying maintainability requirements that tactics must satisfy.

**Adapt:** The SIG/Visser 10 guidelines as concrete measurement targets, but replace proprietary SIG benchmark thresholds with open-source equivalents (Radon for cyclomatic complexity, pylint/flake8 for code smells, coverage.py for test coverage). Map guidelines to specific tactic implementations: e.g., "Separate Concerns in Modules" maps to "Split Module" tactic, "Couple Architecture Components Loosely" maps to "Use Encapsulation" and "Restrict Dependencies" tactics.

**Avoid:** Relying solely on the Maintainability Index (MI) as a system-level metric -- Molnar demonstrates it is confounded by size and lacks OO-specific features. Avoid using pre-ISO quality models (McCall, Boehm, FURPS) as evaluation frameworks, as both Al-Qutaish and Al-Badareen demonstrate their metrics are poorly defined and coverage is incomplete. Avoid treating maintainability as a monolithic attribute -- always decompose into ISO 25010 sub-characteristics for meaningful analysis.

### Related Work Draft
> Software quality models have evolved through successive refinements aimed at decomposing abstract quality concerns into measurable attributes. McCall's 1977 model first established a hierarchical factors-criteria-metrics structure, while Boehm's 1978 model introduced utility-oriented decomposition. Comparative analyses by Al-Qutaish (2010) and Al-Badareen et al. (2011) demonstrate that these early models suffer from inconsistent factor definitions and incomplete metric operationalization, with factor coverage ranging from 20.34% (FURPS) to 63.67% (McCall). The ISO 9126 standard (2001) consolidated international consensus by decomposing maintainability into four sub-characteristics: analyzability, changeability, stability, and testability. Its successor, ISO/IEC 25010 (2011), refined this into five sub-characteristics -- modularity, reusability, analysability, modifiability, and testability -- which we adopt as the evaluation framework for this work.
>
> Operationalizing these sub-characteristics requires concrete metrics and benchmarks. The SIG maintainability model (Visser 2016) provides 10 coding guidelines mapped to specific metrics (unit length, cyclomatic complexity, duplication, interface size, coupling, component balance, codebase size, test coverage, and code cleanliness), with thresholds calibrated against hundreds of real systems via star ratings. These guidelines map directly to the modifiability tactic categories defined by Bass et al. (2021): short and simple units with separated concerns correspond to cohesion tactics, loose coupling guidelines correspond to coupling-reduction tactics, and eliminating duplication corresponds to the "Abstract Common Functionality" tactic. Empirical evidence from Molnar and Motogna (2020), who analyzed 111 releases of three open-source Java applications across a decade, confirms that the SQALE technical debt model -- implemented in SonarQube -- provides the most reliable system-level maintainability measurement, independent of code size, while the Maintainability Index remains useful for finer-grained class-level assessment. Their finding that maintainability effort concentrates in a small subset of packages suggests that targeted architectural interventions can yield disproportionate quality improvements, motivating the approach adopted in this thesis: applying LLM-implemented architectural tactics to identified maintainability hotspots and measuring the resulting change in ISO 25010-aligned quality metrics.
