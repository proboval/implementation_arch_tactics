4
2
0
2

v
o
N
4

]
E
S
.
s
c
[

1
v
0
2
3
2
0
.
1
1
4
2
:
v
i
X
r
a

An Empirical Study on the Code Refactoring Capability of Large Language
Models

JONATHAN CORDEIRO, Queenâ€™s University, Canada
SHAYAN NOEI, Queenâ€™s University, Canada
YING ZOU, Queenâ€™s University, Canada

Large Language Models (LLMs) aim to generate and understand human-like text by leveraging deep learning and natural language

processing techniques. In software development, LLMs can enhance the coding experience through coding automation, reducing

development time and improving code quality. Code refactoring is a technique used to enhance the internal quality of the code base

without altering its external functionalities. Leveraging LLMs for code refactoring can help developers improve code quality with

minimal effort. In this paper, we conduct an empirical study to assess the quality of the refactored code generated by StarCoder2, which

is an LLM designed for code generation. Specifically, we (1) evaluate whether the code refactored by the LLM or by developers is more

effective at improving code quality, (2) understand the differences between the types of refactoring applied by the LLM and developers

and compare their effectiveness, and (3) evaluate whether the quality of the refactored code generated by the LLM can be improved

through one-shot prompting and chain-of-thought prompting. We analyze the refactoring capabilities of StarCoder2 and developers on

30 open-source Java projects. We find that StarCoder2 reduces code smells by 20.1% more than developers on automatically generated

refactorings. StarCoder2 excels in reducing more types of code smells, such as Long Statement, Magic Number, Empty Catch Clause,

and Long Identifier. Developers perform better in fixing complex issues, such as Broken Modularization, Deficient Encapsulation,

and Multifaceted Abstraction. Furthermore, StarCoder2 outperforms developers in refactoring types that are more systematic and

repetitive. However, developers surpass in refactorings that require a deeper understanding of code context and architecture. Our

findings show that One-shot prompting improves unit test pass rate over the zero-shot prompt by 6.15% and reduces code smells

at a 3.52% higher rate. When generating five refactorings per input, StarCoder2 achieves a unit test pass rate of 28.8% higher than

when generating one refactoring per input, which indicates that the combination of one-shot prompting with multiple refactoring

generations per input leads to the best performance. By providing insights into the capabilities and best practices for integrating LLMs

like StarCoder2 into the software development process, our study aims to enhance the effectiveness and efficiency of code refactoring

in real-world applications.

CCS Concepts: â€¢ Software and its engineering;

Additional Key Words and Phrases: Code Refactoring, Artificial Intelligence, Large Language Models, Auto-Generated Code, Code

Quality

ACM Reference Format:

Jonathan Cordeiro, Shayan Noei, and Ying Zou. 2018. An Empirical Study on the Code Refactoring Capability of Large Language

Models. In . ACM, New York, NY, USA, 25 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION

Automatic code generation allows developers to produce code more efficiently and consistently [44]. The advancement

of Large Language Models (LLMs) has demonstrated its ability in generating code snippets and solving complex

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
Manuscript submitted to ACM

1

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

programming problems [12, 49]. LLMs are trained using an enormous amount of open-source code; therefore, if the

training code has flaws, the generated code could suffer from poor design [52] or introduce technical debts [7]. Therefore,

the integration of the generated code into the software development process may increase maintenance costs and

reduce the overall quality of the software [52].

Code refactoring is the process of enhancing the internal quality of the code without altering its external behavior [14,

15, 30]. Refactoring types are the specific techniques used, such as renaming variables for clarity, extracting methods

to reduce complexity, and moving classes or methods [17]. Refactoring is typically associated with the elimination of

code smells [34] which are design flaws that violate design principles and compromise the maintainability [48]. As a

result, the refactored code can improve software design, make code easier to understand, and remove dependencies

among software components [27, 30, 32]. Human developers bring contextual knowledge and experience to refactoring,

which is challenging for automated systems to replicate [14, 15, 48]. Recent studies have explored the potential of

LLMs (e.g., GPT-3. [42]) in automating code refactoring, where models are provided with a prompt and the code to be

refactored [8, 42]. However, there are no systematic studies to understand the impact of LLM-generated refactorings on

code quality improvement [1].

To have a better understanding of how LLMs manage refactoring tasks without compromising the functionality of

the code, we conduct an empirical study to evaluate the effectiveness of LLMs in refactoring tasks and compare their

performance against that of human developers. We aim to provide an evaluation framework to determine how well

LLMs can replicate or surpass human expertise in code refactoring tasks. Given the wide range of refactoring types

that developers can perform, we strive to gain insight into the coverage of refactoring types that LLMs can perform,

compared to those performed by developers. Moreover, we explore the possibility of improving the capabilities of LLMs

in refactoring by applying one-shot and chain-of-thought prompting to enhance LLMsâ€™ performance in refactoring.

The LLMs are trained on public data, including open-source repositories [50]. Therefore, if the LLMs are trained

using refactored code from the developers that we aim to compare LLM-generated refactored code with, it cannot truly

reflect LLMsâ€™ refactoring capabilities. Overfitting could occur if the LLM has been trained on the same code, potentially

leading to artificial inflation of its performance and resulting in over-optimistic results [5]. Popular LLMs such as

OpenAIâ€™s GPT-4 [33], Googleâ€™s PaLM [9], and Metaâ€™s LLaMA [45] do not provide their training datasets publicly. To

mitigate the risk of data leakage, we choose StarCoder2-15B-instruct [24], trained on the publicly available dataset, The

Stack v2 [24]. Therefore, we ensure that our selection of open-source projects for evaluating the LLM has not been

included in the LLMâ€™s training dataset.

StarCoder2 has exceptional performance on the HumanEval benchmark [54] which assesses the modelâ€™s ability to

generate functional code for a variety of programming tasks, making it a strong indicator of the modelâ€™s effectiveness in

real-world coding scenarios. StarCoder2 achieves 46.3% for the pass@1 metric on the HumanEval benchmark, meaning

that 46.3% of the generated code snippets perform the required task correctly on the first attempt.

In this paper, we conduct an empirical study using 30 open-source Java projects, that are not included in the Stack-

v2 [22]. We extract 5,194 refactoring commits from the 30 projects. To compare the refactorings conducted by human

developers and the ones generated by StarCoder2, we leverage the rich commit history available in the subject projects.

We aim to extract the code in a file level before a developerâ€™s refactoring and the code after the developerâ€™s refactoring.

Then we use prompt engineering to instruct StarCoder2 to refactor the code before a developer conducts the refactoring

on the same code. We compare the refactorings produced by StarCoder2 and the ones by developers by analyzing the

reduction of code smells, as well as the impact of different refactoring types on code quality improvement. We aim to

investigate the following four research questions:

2

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

RQ1: Can LLMs outperform developers in code refactoring? We aim to assess whether StarCoder2 can be

used as a reliable solution to automate code refactoring. We compare the distribution of the refactoring operations

performed by StarCoder2 and developers. We evaluate their effectiveness with the reduction of code smells and their

improvement on code quality measured by various code metrics. We observe that StarCoder2 achieves a significantly

higher performance in reducing code smells by 44.36%, compared to a 24.27% reduction rate for the developer. StarCoder2

excels in improving code quality measured by code metrics, often surpassing developers in these areas.

RQ2: Which types of code smells are most effectively reduced by LLMs or developers? To measure the quality

improvement capabilities of StarCoder2 in refactoring, we identify the types of code smells that can be effectively

removed by StarCoder2 or developers. We observe that StarCoder2 outperforms developers to address systematic

and repetitive issues, such as condensing a long statement or shortening a long parameter list. However, developers

show superiority in handling complex, context-dependent code smells, such as correcting a broken modularization or

deficient encapsulation.

RQ3: Which refactoring types are most effective for improving code quality? To understand the capabilities

of StarCoder2 in handling different types of refactorings, we compare the refactoring types that it performs to effectively

eliminate code smells and improve code quality with those performed by developers. Our findings show that StarCoder2

is particularly effective in refactoring types that improve code within a class. Developers on the other hand excel in

refactoring types that involve changes affecting multiple classes.

RQ4: How does prompt engineering affect the quality of LLM-generated refactorings? To explore the impacts

of prompt engineering to improve the capabilities of StarCoder2, we use chain-of-thought and one-shot prompting

techniques. The results show that the one-shot prompting yields the highest unit test pass rate of 34.51%, marking an

improvement of 6.15% over zero-shot prompting, and a smell reduction rate (SRR) of 42.97%, which is an increase over

the zero-shot prompt by 3.52%. The chain-of-thought prompt, where we give the LLM refactoring type suggestions

along with a definition, achieves a 32.22% unit test pass rate and a 42.34% SRR, which improves upon the numbers from

the zero-shot prompting by 3.86% and 2.89% respectively.

Our work makes the following main contributions:

â€¢ We conduct a comprehensive evaluation of the capabilities of StarCoder2 in automated code refactoring.
â€¢ We develop an evaluation framework for measuring the effectiveness of LLMs in code refactoring, with a focus

on code quality improvement and functionality preservation.

â€¢ We compare different prompting techniques, including chain-of-thought and one-shot prompting, to assess their

impact on the generation of high-quality refactorings and offer practical guidance on their application.

â€¢ We provide a replication package to enable the reproducibility of our study. The replication package of the study
can be accessed at: https://github.com/Software-Evolution-Analytics-Lab-SEAL/LLM_Refactoring_Evaluation

Paper Organization. The remainder of our study is organized as follows. Section 2 describes the experiment setup

of this study. Section 3 presents the motivation, approaches, and results of our research questions. Section 4 discusses

the threats to the validity of our findings. Section 5 surveys related studies and compares them to our work. Finally, we

conclude our paper and present future research directions in Section 6.

2 EXPERIMENT SETUP

We start with selecting projects that are not included in the training dataset of StarCoder2. Then we conduct the data

processing to extract the refactoring-related commits, associated files containing refactorings, code smells detection,

3

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Fig. 1. Overview of Our Approach for Data Collection and Answering Research Questions.

and code metrics measurement. Next, we prompt StartCoder2 to generate refactorings on each extracted file in a commit

before the developersâ€™ refactoring operations. Furthermore, we analyze the refactored code generated by StarCoder2

and answer the three research questions. The rest of the section presents the experiment of our study, including our

data collection and data analysis approaches.

2.1 Project Selection

We conduct our study using the 20-MAD dataset [10], which includes 765 Apache projects along with their historical

development information, such as code commit information. We limit our study to projects primarily written in Java.

Selecting projects using Java is based on the strong performance of refactoring detection tools developed for Java [46, 47]

and Javaâ€™s popularity [3, 19] among developers [30], which aligns with our goal of examining refactorings in a language

where refactoring practices are well-established and widely adopted.

To include projects with sufficient historical development and tool support, inspired by previous work [30], we
exclude the projects that: (1) have less than 80% of Java source code; (2) have less than the 1st quantile of commit counts
(i.e., < 1,021 commits); (3) have a short lifespan (i.e., < one-year of commit history); and (4) projects with less than
one-year of commit history as projects with a longer lifespan and more commits are likely to have more refactoring

operations [15]. As a result, we obtain 195 Java projects.

To avoid data leakage, we filter out the 135 Java projects from our initial dataset that are included in the StarCoder2.

To ensure a balanced distribution of refactoring commits across projects, we filter out projects with fewer than the

median number (i.e., 129) of refactoring commits in a final 30 Java projects for our analysis. To obtain the refactoring

information for each commit in the selected projects, we utilize the dataset provided by a previous study [30], which is

comprised of commits from the 195 Java Apache projects mentioned above. We use the commits of the 60 projects from

this dataset for our analysis. The commits contain refactorings detected by RMiner [47], and include 59 different types

of refactorings along with the modified lines of code, which is used to calculate code churn. Code churn refers to the

sum of lines added and removed during a commit [15]. The evaluation of RMiner on this dataset performed in previous

work [30], shows an overall precision of 99.7% and a recall of 94.2%, confirming it as an effective refactoring detection

tool [47].

2.2 Data Processing

2.2.1 Refactoring Commits Selection. To ensure that the developer performs the same task (i.e., refactoring) as the

StarCoder2, rather than other tasks such as development or bug fixing, we select commits that 100% of the lines of code

churn consist of refactoring operations. Refactoring code churn refers to the sum of lines added and removed in the

4

195 JavaProjectsProjectSelection30 JavaProjects   RQ1: Can LLMs outperform developers in code refactoring?   RQ2: Which refactoring types are most effective?RefactoringCommits SelectionUnit Test GenerationCode Smell andMetrics ExtractionData PreparationUnit Test ExecutionCode SmellExtractionCode MetricsExtractionResults Analysis   RQ3: Which code smells types can be reduced?   RQ4: Can LLMs improve refactorings using prompt engineering?RefactoringsGenerationAn Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

process of a refactoring operation. A refactoring commit is identified by comparing the refactoring code churn with the

total code churn within a commit shown in the following equation [30]:

Refactoring Ratio of Commit (i) =

Refactoring Code Churn of Commit (i)
Total Code Churn of Commit (i)

Ã— 100%

(1)

If the code churn is entirely due to refactoring (i.e., Ratio = 100%), then the commit is classified as a refactoring

commit.

2.2.2 Code Smell Extraction. Code smells are indicators of underlying design or implementation issues that may affect

the quality, such as the maintainability and readability of the software [30]. Code refactoring is commonly associated

with the elimination of code smells [6, 43, 46]. We use DesigniteJava 2.5.2 [40] to extract code smells by inputting

the source code of each Java file from before and after each refactoring commit. DesigniteJava offers comprehensive

coverage in detecting code smells across multiple categories [39]. It can detect 46 different types of code smells, including

7 architecture smells, 18 design smells, 9 implementation smells, 4 testability smells, and 8 test smells.

â€¢ Architecture smells describe the characteristics in the systemâ€™s architecture that indicate potential issues affecting
the overall software quality. For example, unstable dependencies code smell describes the case that relies on

frequently changing modules.

â€¢ Design smells indicate poor adherence to design principles that can negatively impact the softwareâ€™s modularity,
flexibility, and reusability. These smells suggest that the design may not support future changes or scale effectively,

leading to increased technical debts and higher maintenance costs. For example, unnecessary abstraction describes

the code smell with too many layers or indirections that could obscure the codeâ€™s intent.

â€¢ Implementation smells are signs in the source code to flag problematic code that could make the code harder to
maintain, understand, or extend. Common examples include large classes, repeated code, or very long methods,

which indicate that the code may need improvement to enhance its quality and ease of maintenance.

â€¢ Testability smells refer to the degree to which the development of test cases can be facilitated by the software

design choices. Examples include hard-wired dependencies and excessive dependency.

â€¢ Test smells are resilted from bad programming practices in unit test code indicating potential design problems in

the test code. Examples include empty test, unknown test, and constructor initialization.

2.2.3 Code Metrics Computation. Code metrics provide valuable insights into the quality of the code, such as main-

tainability, and complexity [31]. We collect code metrics that are used to evaluate complexity, cohesion, coupling,

and modularity. The description of the collected code metrics is listed in Table 1. Modularity metrics help assess the

degree to which the system is divided into independent components, which promotes ease of maintenance and testing.

Coupling metrics indicate the degree of interdependence between classes, where lower coupling is preferred for better

modularity and flexibility. Cohesion metrics measure how closely related the responsibilities of a class are, with higher

cohesion typically leading to improved code clarity and design quality [35]. We use the Understand tool [36], which is a

static code analysis tool to extract code metrics.

2.2.4 Tests Preparation. Code refactoring should not alter the external functionality of the code [15]. To evaluate

whether the refactorings generated by StarCoder2 retain their original functionality, we execute unit tests on the

refactored code. As some projects donâ€™t have test cases, we cannot obtain the original test cases from all refactoring

commits (identified in Section 2.2.1). Therefore, we generate test cases for all the refactoring commits to test whether

the refactored code alters functionalities or introduces new defects. We utilize EvoSuite [16], which automatically

5

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Table 1. Description of Metrics Used in the Study [37].

Metric

Description

Quality At-
tribute

Rationale for Inclusion

Count Class
Coupled

Number of other classes coupled to.

Coupling

Helps identify tightly coupled classes, indicating potential areas for
refactoring to reduce complexity.

Count Class
Coupled Modified

Number of other non-standard classes coupled
to.

Coupling

Focuses on the complexity introduced or modified during changes, rele-
vant for assessing refactoring impact.

Count Class
Derived

Count Decl
Class Variable

Count Decl
Instance Variable

Percent Lack
of Cohesion

Percent Lack of
Cohesion Modified

Avg Cyclomatic

Cyclomatic

Max Cyclomatic

Sum Cyclomatic

Number of immediate subclasses.

Modularity

The number of class-level variables declared in a
class.

The number of instance variables declared in a
class.

The percentage of methods in a class that do not
share instance variables.

A variation of PercentLackOfCohesion, modified
for accessor methods.

The average cyclomatic complexity for all nested
functions or methods.

The McCabe cyclomatic complexity of a single
method or function.

The maximum cyclomatic complexity of any
method in a class or project.

The total cyclomatic complexity of all methods
in a class or project.

Modularity

Modularity

Cohesion

Cohesion

Complexity

Complexity

Complexity

Complexity

High values may indicate a deep inheritance hierarchy, affecting main-
tainability and ease of modification.

A large number of class variables can reduce cohesion and increase the
potential for errors.

High instance variable counts may signal a class doing too much, reduc-
ing its maintainability.

Indicates the degree of cohesion in a class, with higher values suggesting
poor design and low cohesion.

Assesses the impact of changes on class cohesion, useful for understand-
ing the effects of refactoring.

Provides an overall view of the codeâ€™s complexity, helping to identify
areas that may need simplification.

Directly measures the complexity of a method, indicating potential
difficulties in testing and understanding.

Highlights the most complex methods, which could be refactoring tar-
gets to improve maintainability.

Summarizes the overall complexity, aiding in assessing the overall main-
tainability and testability of the codebase.

generates test suites that are optimized to maximize code coverage across multiple criteria, including line coverage

(i.e., ensuring all lines of code are executed), branch coverage (i.e., testing each possible path in the code), and output

coverage (i.e., verifying the correctness of program outputs). EvoSuite incorporates JUnit 4 assertions, which are used

to capture and validate the current behavior of the tested classes. We take the following steps to generate test cases

using EvoSuite:

(1) Create a project for the code extracted from each refactoring commit: For each refactoring commit, we set

up a project that contains only the files modified by the commit in their state before the refactoring was performed.

We ensure the project is configured with the necessary dependencies, either by extracting the pom.xml from the

repository or generating one based on a template that includes essential libraries (e.g., JUnit and Mockito) for

unit testing. A pom.xml file is an XML configuration file used by Apache Maven to manage project dependencies,

build configurations, and other essential project information. This ensures that the project can be compiled and

executed for further analysis. Since only files changed in the commit are included, some dependencies may be

missing. To handle this, Mockito [29] is used to create mock objects for unavailable dependencies, allowing tests

to execute effectively. A bash script, included in our replication package, retrieves or creates the pom.xml and

builds the Java project using Apache Maven [28]. EvoSuite then builds the test cases based on the pom.xml.

(2) Compile the project: After setting up the project for each commit, we compile the code using Apache Maven

to ensure that it builds successfully. Compiling the project verifies that all dependencies are correctly set up and

that the source code is free of compilation errors, which is crucial before generating test cases.

(3) Generate test cases with EvoSuite: Once the project is compiled, we use EvoSuite to automatically generate

unit tests. EvoSuite is run on each Java class within the project. For every class, EvoSuite generates a suite of

tests designed to cover different behaviors and edge cases of the code. These generated tests are stored in a

separate directory within each commit folder, providing a structured and isolated location for the test cases.

6

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

We manually validate the generated unit tests to ensure their functionality as EvoSuite may run into errors when

generating unit tests and some unit tests may be incomplete. We randomly select 381 unit tests from 39,309 generated

unit tests to assess each one to determine if it is valid and functional. Of the 381 unit tests, 348 tests (91.3%) were valid

and functional, meaning they executed as expected without encountering the above issues. After this validation, we

filter out unit tests that match patterns of failing tests as follows:

â€¢ Tests that fail due to missing the required dependencies - improper configuration in the initial states.
â€¢ Overly strict assertions, where minor variations in outputs or system state that do not affect functionality cause

the test to fail.

2.3 Hardware Environment

Our experiment is conducted using 4 Nvidia A100 GPUs on a server, each with 80 GB of memory. The operating system

of the server is Ubuntu 22.04.4 LTS. It takes approximately 6 days to generate refactorings for 5,194 commits, and takes

around 1.6 minutes per commit. The GPUs are utilized for StarCoder2 model inferencing.

3 RESULTS

In this section, we provide the motivation, approach, and findings for each of our research questions.

3.1 RQ1. Can LLMs outperform developers in code refactoring?

3.1.1 Motivation. Developers typically perform refactoring by manually analyzing and editing code to improve

maintainability and readability [14, 15]. This requires developers to rely on their expertise and knowledge of best

practices, which can make the process both time-consuming and prone to errors [27]. In contrast, LLMs generate

refactorings by processing the prompt and code through pre-trained models, using large-scale datasets and patterns

learned from vast amounts of code to suggest improvements automatically. This research question aims to evaluate

whether LLMs, particularly StarCoder2, can outperform developers in refactoring tasks by comparing the quality of

refactorings generated by StarCoder2 and developers.

3.1.2 Approach. To compare refactorings generated by StarCoder2 with those performed by developers, we first

select a set of refactoring commits from developers. Next, we generate refactorings using StarCoder2. Finally, we use

code quality metrics to compare the quality of the code refactored by developers with that refactored by StarCoder2.

Each step of this process is detailed as follows:

Commit selection: To set a foundation for comparison, we establish the developerâ€™s refactored code as the baseline.

We only use commits with 100% of the code churn consisting of refactoring operations, ensuring that the changes made

by developers are solely for refactoring purposes and do not include any other development activities, such as adding

new features. The selected commits represent valid refactoring operations, as they are accepted pull requests and pass

the unit tests generated for code before the refactoring. Generating refactorings on all available commits would be

computationally expensive and time-consuming. From our 30 projects mentioned in Section 2.1, we select a sample size

of commits with a confidence level of 95% and a margin of error of 5%, ensuring a representative sample from each

project [18], to generate refactorings using StarCoder2. By using a sample size, we can maintain confidence in the

evaluation results while balancing the trade-off between computational feasibility and accuracy. Therefore, we have

5,194 commits containing a total of 39,309 files for testing the model.

7

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Refactoring generation: We use the code segment containing the code before refactoring as input to the LLMs,

so we can use the generated code from the LLM to compare with the refactored code by the developer on the same

snippets of code in a commit. The prompt comprises snippets of code across the files in a commit.

Instruction
You are a powerful model specialized in refactoring Java code. Code refac-
toring is the process of improving the internal structure, readability, and
maintainability of a software codebase without altering its external behav-
ior or functionality. You must output a refactored version of the code.

Prompt
# unrefactored code snippet(java):
{code_segment_before_refactoring}

# refactored version of the same code snippet:

Fig. 2. Zero-shot Prompt Used to Instruct StarCoder2 to Conduct Refactoring

Figure 2 shows the template zero-shot prompt, which is a method where the model receives no additional examples

or guidance beyond the initial instruction [20], used to instruct StarCoder2 to take the developersâ€™ code snippet before

refactoring as input. We employ the "#" symbol as a delimiter to ensure the LLM focuses on the instructions even with a

block of code in the prompt [53]. The LLM is specifically prompted to refactor Java code and we break the code snippets

into chunks (i.e., one chunk for each file in a commit). We create a zero-shot prompt for each refactoring commit using

a Python script, which is loaded onto an Nvidia A100 GPU.

Test Pass Rate Evaluation: We use Pass@1, Pass@3, and Pass@5, all using the same zero-shot prompt, to assess

the functional correctness of the refactoring operations:

â€¢ pass@1: We generate a single refactored solution for each code snippet in a commit. This solution is evaluated by
executing the corresponding unit tests. If the refactored solution passes the unit test, it is considered a successful

refactoring under the Pass@1 criterion.

â€¢ pass@3: We generate three refactored solutions for each code snippet in a commit. Each solution is independently
evaluated by running the corresponding unit tests. If at least one of these three refactored versions passes all

unit tests, the refactoring is considered successful under pass@3.

â€¢ pass@5: We generate up to five refactored solutions for each code snippet in a commit. As with pass@3, we run
the unit tests on each of the five refactored versions. If at least one solution successfully passes all unit tests, the

refactoring is deemed successful under pass@5.

In pass@3 and pass@5 scenarios, if multiple refactoring solutions pass the unit test, we select the best solution by

analyzing which solution reduces the most amount of code smells. If two or more solutions pass the unit tests and

reduce the same amount of code smells, we select the best solution based on which one has the highest improvement in

code metrics (i.e. cohesion, coupling, modularity, and complexity).

To assess the effectiveness of StarCoder2 in improving overall code quality after passing unit tests, we use two cate-

gories of measurements: code smells and code metrics. For code smell reduction and code quality metrics improvement,
we calculate the improvement rate using the following formula. Let ð´before represent the value of the attribute (code
smells or code metric) before refactoring, and ð´after represent its value after refactoring. The improvement rate (IR) is
8

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 2. Comparison of Unit Test Pass Rates and SRR Across the 30 Projects

Refactoring

LLM

Developer

Pass@1
Pass@3
Pass@5
-

Unit Test Pass Rate
Median
26.8%
47.0%
55.4%
100%

28.4%
48.5%
57.2%
100%

37.5%
39.6%
43.2%
23.5%

Average
39.5%
40.8%
44.4%
24.3%

Average Median

SRR

calculated as follows:

ð¼ð‘… =

ð´before âˆ’ ð´after
ð´before

Ã— 100

(2)

We collect code metrics from three versions of the code using the static analysis tool, Understand [36]: (1) the

unrefactored code, to assess its quality before refactoring; (2) the developer-refactored code, to evaluate its quality after

developer refactoring; (3) and the LLM-refactored code, to assess its quality after LLM refactoring. This analysis allows

us to systematically compare the effectiveness of refactorings carried out by developers and the LLM respectively. To

examine the quality of refactorings produced by the LLM and developers, we examine the following null hypothesis:

â€¢ H0: The refactoring performed by the LLM is as effective as those performed by human developers in reducing code

smells and improving code metrics.

We test H0 by comparing unit test pass rates, code smell reduction rates, and code metric improvement rates between
the LLM and developers. We perform a Mann-Whitney U-test [25] on the distributions of code smell reduction rates

across the 30 projects between developers and the LLM, using a confident level of 5% (i.e., p-value<0.05). The U-test

assesses whether two or more samples originate from the same distribution. It does not assume a normal distribution

since it is a non-parametric statistical test. We also perform the U-test on the distributions of code metric improvement

percentages across the 30 projects for the LLM and developers.

We quantify the differences in the effect size using Cliffâ€™s delta (ð›¿) [26], which measures the degree of overlap
between the distributions from StarCoder2 and developers. A higher effect size indicates a larger magnitude of the

differences between the two approaches for that particular type of code smell. A Cliffâ€™s delta greater than 0.15 indicates

a small effect size, between 0.33 and 0.47 indicates a medium effect size, and greater than 0.47 indicates a large effect

size [26].

3.1.3 Findings. Refactorings generated by StarCoder2 can sometimes propose changes that may affect the

functionality of the code. To provide a detailed comparison of the refactoring performance between StarCoder2

and developers, as shown in Table 2, we present the unit test pass rates alongside the code smell reduction rate for

pass@1, pass@3, and pass@5 scenarios. Developers achieve a 100% unit test pass rate as all of their refactorings are

accepted pull requests and maintain the same functionality of the code, while StarCoder2â€™s performance varies across

the pass metrics. StarCoder2 achieves a 28.36% pass@1 for our experiments. We show the median unit test pass rates

(i.e., pass@1, 3, 5) of the selected projects in Figure 4. StarCoder2 refactored code has an average unit test pass rate of

the 30 projects of 57.15% for the pass@5 metric. There is an average of 20.1% improvement in unit test pass rate from

pass@1 to pass@3 and an average of 8.7% improvement in unit test pass rate from pass@3 to pass@5. The refactored

code generated by StarCoder2 achieves a 57.15% unit test pass rate at Pass@5, indicating substantial improvement

over Pass@1 (28.36%) and Pass@3 (8.7%). Therefore, it is recommended to have comprehensive testing when

utilizing LLMs like StarCoder2 to ensure the correctness and integrity of the refactored code.

9

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Fig. 3. Distribution of Unit Test Pass Rates Across 30 Software
Projects After StarCoder2-Generated Refactorings.

Fig. 4. Distribution of Code Smells Across 30 Software Projects
After LLM-Generated Refactorings.

Fig. 5. Distribution of Code Smell Reduction Rates Across 30
Projects for StarCoder2-Generated Refactorings and Developer
Refactorings.

StarCoder2 has better performance in code refactoring than developers when the generated code can pass

the unit test. Figure 3 shows the distribution of the total code smells before and after the application of developer

refactoring and LLM refactoring in the 30 projects. The initial dataset representing unrefactored code contains 17,429

code smells, indicating several potential areas for improvement. Upon analyzing developer-driven refactoring, we

observe 13,199 code smells, a code smell reduction rate of 24.27%. For code that StarCoder2 passes unit tests for, there is

an initial 12,213 code smells before refactoring; we do not include code smells to calculate code smell reduction from code

that StarCoder2 cannot successfully generate refactorings for (i.e., the refactorings pass unit tests). StarCoder2 reduces

the initial number of code smells from 12,213 to 6,795 showing a code smell reduction rate of 44.36% for 30 projects.

We show the distribution of code smell reduction rates across the 30 projects for both StarCoder2 and developers in

Figure 5. StarCoder2 reduces code smells at a 20.1% higher rate than developers in our experiments. The results of the

U-test for code smell reduction show a p-value of 0.003, indicating a significant difference in the distribution of code

smells per commit before refactoring, after developer refactoring, and after LLM refactoring. StarCoder2 reduces code

10

Pass @1 (%)Pass @3 (%)Pass @5 (%)Unit Test Pass Rate3040506070Percentage of Passed Unit TestsBefore RefactoringLLM RefactoringDeveloper RefactoringRefactoring Method2004006008001000Code Smell CountLLM ReductionDeveloper ReductionRefactoring Method010203040506070Reduction Rate (%)An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 3. Comparison of Improvement in Metrics (%) Over 30 Projects Between StarCoder2 and Developers. Cliffâ€™s Delta is shown for
the Code Quality Metrics that are Significantly Improved by either StarCoder2 or Developers

Quality Attribute

LLM (Avg)

LLM (Median) Dev (Avg) Dev (Median) Cliffâ€™s Delta (Interpretation)

Metric

CountClassCoupled

CountClassCoupledModified

CountClassDerived

CountDeclClassVariable

CountDeclInstanceVariable

PercentLackOfCohesion

AvgCyclomatic

Cyclomatic

MaxCyclomatic

SumCyclomatic

Average

PercentLackOfCohesionModified

Cohesion

Coupling

Coupling

Modularity

Modularity

Modularity

Cohesion

Complexity

Complexity

Complexity

Complexity

21.4

18.3

17.9

12.5

19.7

22.8

23.9

17.4

16.5

15.8

18.6

-

19.32

20.1

17.5

16.8

11.8

18.9

21.7

22.8

16.3

15.4

14.9

17.4

18.2

24.1

16.8

15.2

14.9

17.0

20.4

21.5

14.6

13.5

13.3

15.9

17.46

23.5

16.2

14.7

14.4

16.5

19.9

20.7

14.0

13.0

12.8

15.1

16.9

-

-

-

-

-

-

0.42 (Medium)

0.45 (Medium)

-

-

0.47 (Medium)

-

smells by 44.36%, outperforming developers who achieve a 24.27% reduction, making StarCoder2 20.1% more effective

in improving code quality through smell reduction.

Upon analyzing the 4.91% increase in code smell reduction rate from Pass@1 to Pass@5, we observed a notable rise

in the reduction rates of the Rebellious Hierarchy Smell (5.18%), along with improvements in reducing Long Method

(8.76%) and Long Statement Smells (6.32%). The reduction rates for all other code smells remained consistent across

Pass@1, Pass@3, and Pass@5. This suggests that regenerating refactorings multiple times can enhance the

effectiveness of code smell reduction, as the LLM may hallucinate for some refactoring generations that

attempt to address complex code smells.

StarCoder2 excels in improving key code metrics related to cohesion, modularity, and complexity, often

surpassing developers in these areas. The results presented in Table 3 provide a quantitative analysis of the

refactoring capabilities of StarCoder2 compared to developers, where a higher percentage reduction indicates a

greater ability to improve code quality. StarCoder2 achieves a 19.7% average reduction in instance variables (i.e.,

CountDeclInstanceVariable), outperforming developers, who achieve a 17.0% reduction. Similarly, StarCoder2 leads in

reducing PercentLackOfCohesion by 22.8% compared to 20.4% for developers. These results suggest that StarCoder2

is particularly effective at reducing code elements that contribute to poor cohesion, leading to more modular and

well-structured code. StarCoder2 shows a significant improvement in reducing PercentLackOfCohesionModified, with

a 23.9% reduction compared to 21.5% for developers, demonstrating a moderate effect size of 0.42 (Cliffâ€™s Delta). This

indicates that StarCoder2 consistently improves cohesion after modifications, more so than developers. StarCoder2

excels in reducing cohesion-related code metrics, such as instance variables and cohesion percentages, outperforming

developers in these areas. It shows a 19.7% reduction in instance variables and a 22.8% reduction in cohesion issues,

indicating better code modularity and structure. However, developers outperform StarCoder2 in reducing the number

of coupled classes (i.e., CountClassCoupled and CountDeclClassVariable), with average reductions across 30 projects

of 24.1% and 14.9%, respectively. This suggests that while StarCoder2 excels in several areas, developers may

still have an edge in refactoring tasks that require a deep understanding of class dependencies and variable

declarations.

When examining cyclomatic complexity, StarCoder2 once again outperforms developers. The model reduces

AvgCyclomatic by 17.4% and SumCyclomatic by 18.6%, compared to reductions of 14.6% and 15.9% by developers,

11

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

respectively. Notably, the reduction in SumCyclomatic has a medium effect size of 0.47, highlighting StarCoder2â€™s

effectiveness in simplifying code logic across entire projects. The modelâ€™s ability to reduce complexity on both individual

methods and overall project levels suggests that it may contribute to easier maintainability over time. We reject the null
hypothesis H0 as StarCoder2 performs significantly better in code smell reduction and various code quality metric
improvements compared with developers.

We use the best refactoring generation from pass@5 to answer the remaining research questions. Pass@5 shows

better performance than pass@1 and pass@3 in passing unit tests and therefore results in more valid refactorings

to analyze. Taking the best refactoring generation from pass@5 results in more data for analyzing refactoring types

performed by the LLM as well as code smell types reduced by the LLM.

Summary of Results

StarCoder2 outperforms developers across most evaluated code metrics, achieving an average reduction across

all metrics of 19.32% compared to 17.46% for developers, demonstrating its strength in automating complex

refactoring tasks. StarCoder2 improves unit test pass rates significantly, reaching 57.15% at Pass@5, though

developers maintain a 100% pass rate. StarCoder2 excels in code smell reduction, reducing smells by 44.36%,

which is 20.1% higher than developers, and particularly effective in addressing smells like long methods

and rebellious hierarchy. It also shows superior performance in reducing cohesion and complexity metrics,

achieving greater modularity and structure. However, developers outperform StarCoder2 in reducing class

coupling, highlighting their advantage in tasks requiring a deeper understanding of class dependencies. Despite

this, StarCoder2â€™s ability to simplify code logic and reduce cyclomatic complexity makes it a valuable tool for

improving code maintainability.

3.2 RQ2. Which types of code smells are most effectively reduced by LLMs or developers?

3.2.1 Motivation. Quality refactorings are linked with the elimination of code smells [6, 43, 46]. These smells can

range from simple syntactic issues, such as Long Statement and Empty Catch Clause, to more complex design flaws,

such as Insufficient Modularization and Multifaceted Abstraction. Understanding which types of code smells are most

effectively reduced by developers or by LLMs is crucial for identifying their strengths and limitations. Developers have

domain knowledge and an understanding of software design principles, while LLMs can apply their extensive training

on code patterns and idioms to address repetitive and rule-based smells. Given that different types of code smells

vary in complexity and context dependency, this research question aims to explore how well LLMs and developers

perform across various categories of code smells. By investigating which code smells each refactoring approach excels

at reducing, we can better understand the strengths of the LLM versus human developers in code refactoring.

3.2.2 Approach. To analyze the effectiveness of refactorings performed by developers and StarCoder2 in reducing

different types of code smells, we compare the performance of StarCoder2 and developers in code smell reductions and

quantify the differences for each specific type of code smell. Specifically, we examine each commit that contains at least

one instance of a code smell, ensuring meaningful comparisons. The detailed steps conducted are provided below:

Comparison of differences: Since our data does not follow a normal distribution, we use the Mann-Whitney

U-test [25] to examine if the distributions of reductions between StarCoder2 and developers originate from different

distributions. The U-test evaluates whether one distribution tends to have higher values than the other, based on the

12

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 4. Comparison of Code Smell Reductions Between StarCoder2 and Developers.

Code Smell Category Code Smell

Better Reduction Cliffâ€™s Delta (ð›¿)

Interpretation

Design

Unutilized Abstraction

Unnecessary Abstraction

Broken Hierarchy

Cyclic-Dependent Modularization

Broken Modularization

Deficient Encapsulation

Multifaceted Abstraction

Insufficient Modularization

LLM

LLM

LLM

LLM

Developer

Developer

Developer

Developer

Implementation

Long Statement

Magic Number

Empty Catch Clause

Complex Conditional

Long Parameter List

Long Identifier

Complex Method

Missing Default

LLM

LLM

LLM

LLM

LLM

LLM

LLM

Developer

0.5366

0.5130

0.5608

0.4815

0.6417

0.5587

0.4636

0.6768

0.5406

0.6310

0.6018

0.5355

0.5703

0.5669

0.5271

0.4488

Large

Large

Large

Large

Large

Large

Medium

Large

Large

Large

Large

Large

Large

Large

Large

Medium

rank of each data point. Each data point in the distribution represents the reduction of a specific code smell within a

single project (i.e., the reduction rate after refactoring by either StarCoder2 or developers for that code smell within
that project). Furthermore, we quantify the differences in the effect size using Cliffâ€™s delta (ð›¿) [26] as discussed in
Section 3.1.2.

3.2.3 Findings. The LLM outperforms developers in reducing 10 of the 16 types of code smells.As illustrated

in Table 4, StarCoder2 can reduce code smells in 7 out of 8 types of code smells in the implementation category with a

significantly large effect size comparing the reduction of code smells. In particular, StarCoder2 excels in addressing

syntactic and pattern-based smells, such as Long Statement, Long Parameter List, Long Identifier, Empty Catch Clause, and

Magic Number, which follow more regular idiomatic, and repetitive patterns. An explanation of each of the code smells

found in the table is provided [38]. This finding suggests that StarCoder2 is particularly effective at addressing

issues related to implementation, where structured, rule-based corrections can be applied.

Developers show a better performance in reducing complex and context-sensitive code smells, particularly

those related to modularization. As shown in Table 4, developers outperform the LLM in reducing Missing Default,

Insufficient Modularization, and Multifaceted Abstraction, which require a deeper understanding of the overall software
architecture and implementation principles. The large effect sizes, as indicated by the Cliffâ€™s Delta (ð›¿) values, for these
code smells show that developers have a significantly large advantage for handling complex design code smells.

In the design category, StarCoder2 outperforms developers in 4 out of 8 design-related code smells (i.e.,

Unutilized Abstraction, Unnecessary Abstraction, Broken Hierarchy, and Cyclic-Dependent Modularization).

The LLMâ€™s effectiveness in reducing design-related smells suggests that it can eliminate unnecessary complexity and

optimize code structure when the issues are relatively straightforward. However, developers outperform StarCoder2

13

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

in addressing more intricate design problems like Deficient Encapsulation and Broken Modularization, which require

deeper reasoning about class dependencies, encapsulation, and how components interact at a design level.

Summary of Results

Our findings suggest that while StarCoder2 performs better at handling implementation-level code smells and

some straightforward design issues, such as Long Statement, Magic Number, Long Identifier, etc., developers are

better at handling more complex, context-dependent code smells, particularly those related to modularization

and encapsulation. This points to the potential value of leveraging the strengths of both LLMs and developers

for comprehensive code smell reduction and to improve overall software quality.

3.3 RQ3. Which refactoring types are most effective by LLMs or by developers for improving code quality?

3.3.1 Motivation. Refactoring types refer to the specific categories of code modifications aimed at improving the

structure, readability, and maintainability of software without altering its external behavior. Examples include renaming

variables for clarity, extracting methods to reduce code duplication, or reorganizing classes to better align with design

principles. In this research question, we aim to understand the differences in the application of various refactoring types

as quality improvements are achieved by both developers and StarCoder2. This comparison enables us to determine

whether StarCoder2 or developers focus on different refactoring types and highlights their respective strengths in the

application of different refactoring types. This information can guide which refactoring techniques should be prioritized

in automated tools and development practices.

3.3.2 Approach. To analyze the different types of refactorings applied and assess their impact on code quality, we

first collect the refactorings performed by StarCoder2 and developers. We then analyze the statistical differences in

refactoring types and their effects on code quality between StarCoder2 and developers. The steps of this analysis are

detailed below.

Quality and Impact Measurement: We use RMiner 3.0 [46] to extract all the refactorings applied by both StarCoder2

and developers across all projects. RMiner is the most comprehensive refactoring detection tool for Java, capable of

identifying up to 102 refactoring types, with a precision of 99.8% and a recall of 98.1% [46, 47]. Then, we examine the

distribution of frequency of each refactoring type performed across all projects together to understand any differences in

applying refactoring strategies between the LLM and developers. Next, we analyze whether StarCoder2 and developers

perform and prioritize different refactoring types. By calculating the code smell reducrtion rate, we evaluate the

effectiveness of the refactorings performed by developers and StarCoder2 in targeting specific types of smells. This

analysis reveals whether LLM-driven refactorings and developer-driven refactorings focus on reducing the same types

of code smells or whether their strategies differ in terms of addressing different issues within the code. Moreover, we

assess how refactorings performed by StarCoder2 and developers impact complexity, cohesion, coupling, and modularity.

By comparing the changes in these metrics before and after refactoring, we aim to determine whether StarCoder2

and developers emphasize different aspects of code quality improvement such as reducing complexity or improving

modularity. This analysis helps identify whether LLMs prioritize improvements in certain areas of code quality more

than developers, or vice versa.

Significance and Size Difference Measurement: To evaluate whether the differences in refactoring types per-

formed between StarCoder2 and developers are statistically significant, we perform a Mann-Whitney U-test [25].

14

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Each data point in our analysis represents either the reduction in a specific code smell for each refactoring type after

refactoring by StarCoder2 or by developers. Additionally, we analyze improvements in code quality metrics following

refactoring by both StarCoder2 and developers. We first use the U-test to check whether the distributions of code smell

reduction between StarCoder2 and developers are statistically different. If the U-test reveals a significant difference

(p-value < 0.05), we conclude that refactorings performed by StarCoder2 and the developer significantly target different

areas of the code, indicated by the different types of code smells. To further quantify the extent of this difference, we
apply Cliffâ€™s delta (ð›¿) [26] to measure the effect size. We discuss small, medium, and large effect sizes in Section 3.1.2. A
large Cliffâ€™s delta value indicates that StarCoder2 and developers exhibit substantial differences in their code smell

reduction, while a smaller value suggests a more subtle difference between the two approaches. We take the same

approach when evaluating the impact of StarCoder2 or developer refactoring on code quality metric improvement.

Fig. 6. Distribution of Frequencies of all Refactoring Types Used by StarCoder2 Across 30 Projects.

3.3.3 Findings. There are distinct preferences in performing various refactoring types between StarCoder2

and developers. Figure 6 shows the distribution of all refactoring types StarCoder2 performs. StarCoder2 demonstrates

a higher frequency of certain refactorings which can be detected by following syntactic rules, such as Rename Method

and Extract Method, compared to developers. Conversely, developers more frequently perform refactorings that require

more checks on dependencies and cause a larger impact on code change propagation, such as Move Method and Change

Attribute Access Modifier.

15

Add Attribute ModifierAdd Class AnnotationAdd Class ModifierAdd Method AnnotationChange Attribute TypeChange Class Access ModifierChange Return TypeChange Variable TypeModify Class AnnotationMove And Rename ClassMove AttributeMove ClassRemove Class AnnotationRemove Class ModifierRemove Method AnnotationRename ClassRename PackageRename ParameterRename VariableMove MethodChange Type Declaration KindMove And Rename MethodRemove Attribute ModifierRemove Thrown Exception TypeEncapsulate AttributeMove PackageChange Method Access ModifierChange Attribute Access ModifierRefactoring Types0100200300400500600700Refactoring Count Across ProjectsComparison of Refactoring Types (LLM vs Developers) for LLM-Performed RefactoringsTypeLLMDeveloperConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Table 5. Comparison of Different Refactoring Typeâ€™s Effect on Code Smell Reduction Between StarCoder2 and Developers

Refactoring Type

Better Reduction Cliffâ€™s Delta (ð›¿)

Interpretation

Remove Class Annotation
Move Class
Add Method Annotation
Remove Method Annotation
Rename Class
Move Attribute
Modify Class Annotation
Add Class Annotation
Move And Rename Method
Change Attribute Type
Move And Rename Class

LLM
LLM
LLM
LLM
LLM
Developer
LLM
LLM
LLM
Developer
LLM

0.6969
0.6060
0.9117
0.9619
0.5871
0.7832
0.9589
0.8539
0.9394
0.7796
0.5048

Large
Large
Large
Large
Large
Large
Large
Large
Large
Large
Large

Table 6. Comparison of Different Refactoring Typesâ€™ Effect on Code Metrics Improvement

Refactoring Type

Better Improvement Cliffâ€™s Delta (ð›¿)

Interpretation

Rename Class
Add Class Annotation
Encapsulate Attribute
Change Return Type
Change Attribute Access Modifier

LLM
LLM
Developer
LLM
Developer

0.6123
0.8517
0.6798
0.8032
0.7215

Large
Large
Large
Large
Large

Table 5 presents a comparison of refactoring types performed by either StarCoder2 or developers that significantly

reduce code smells more than the other (p-value <0.05). The data shows that StarCoder2 performs significantly better in

reducing code smells for refactoring types such as Remove Class Annotation, Remove Method Annotation, and Rename
Class. The Cliffâ€™s Delta (ð›¿) values for these types indicate a large effect size. StarCoder2 is particularly effective at
eliminating annotations and renaming classes in ways that reduce code smells more efficiently than human developers.

This is evidenced by the large effect size for Move Attribute and Change Attribute Type. The developers perform better

at the attribute level which involves data dependencies; these refactorings often require a deeper understanding of the

codeâ€™s context and architecture. Developers tend to engage in more types of refactoring that involve dependency

checks and have a greater impact on the code, demonstrating superior performance in managing complex

structural changes.

To highlight the differences in refactoring patterns between StarCoder2 and developers, we categorized the refactor-

ings and observed distinct focuses.

â€¢ Access control refactorings, which are changes to access modifiers and encapsulation of attributes, are
unique to StarCoder2, which frequently modifies access levels and encapsulates attributes, addressing

visibility and encapsulation issues in a rule-based manner. For instance, StarCoder2 performs refactorings

like Change Method Access Modifier and Encapsulate Attribute, focusing on improving data hiding and access

control.

â€¢ Developers handle more complex method and attribute refactorings such as Extract Method and
Inline Variable, which involve restructuring code for better modularity and readability. These types

of refactorings require a deeper understanding of code context. Class-level refactorings also show divergence,

with StarCoder2 applying simpler changes like Change Type Declaration Kind and Add Class Modifier, while

developers performed more intricate refactorings like Extract Superclass and Pull Up Method, requiring a deeper

understanding of class hierarchies. Additionally, package and file movement refactorings are handled differently:

StarCoder2 focuses on high-level tasks such as Move Package, while developers engage in more granular changes

such as Move Source Folder.

16

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

The full details of all refactorings unique to StarCoder2 and unique to developers are included in the replication package

Summary of Results

StarCoder2 excels in automated, syntactic refactoring types, while developers focus on more complex, structural

changes, demonstrating complementary strengths between the two approaches. A combined approach leverag-

ing both LLMs and developers could potentially offer the most comprehensive strategy for code refactoring.

3.4 RQ4. How does prompt engineering affect the quality of LLM-generated refactorings?

3.4.1 Motivation. Our results from the third research question show that developers often apply more refactoring types

that reduce code smells and improve code quality, but LLMs perform fewer refactoring types than developers. By using

prompt engineering [53], which involves designing the input prompts to guide the LLMâ€™s responses more effectively,

we aim to enhance the LLMâ€™s ability to perform more complex and a wider variety of refactoring typesâ€”specifically

for refactoring types that the LLM either do not perform or perform less frequently compared with developers. The

objective is to determine whether modifying the instructions or providing additional context in the prompt can

encourage StarCoder2 to execute more complex refactoring types, typically performed by developers, thus improving

the code quality.

3.4.2 Approach. To enhance StarCoder2â€™s refactoring experience, we aim to evaluate different prompting techniques

and identify the most effective ones for improving code quality. In the following, we describe our approach.

Prompt strategy selection: We begin by selecting prompting strategies that vary in their level of detail, guidance,

and contextual information. We first identify refactoring types that are consistently performed by developers but are

missing in StarCoder2â€™s output as the results from RQ3 (Section 3.3.3). Once we identify these refactoring types, we

design specific prompts to guide StarCoder2 in applying them. These prompts are categorized as follows:

â€¢ Zero-shot Prompt (baseline): This prompt is the baseline used in RQ1. We compare the performance of the

following prompts with this baseline shown in Figure 2.

â€¢ Chain-of-thought: Shown in Figure 7, this prompt provides suggestions for refactorings, along with the expla-
nation of that refactoring type, by including common refactoring patterns developers perform that the LLM

does not. For example, the prompt may instruct, "Consider using Extract Method or Inline Method," guiding

StarCoder2 toward specific types of refactorings that developers perform with more efficiency.

â€¢ One-Shot Prompt: Shown in Figure 8, in this approach we provide an example of a correctly refactored code
snippet by the developer before asking StarCoder2 to refactor a new piece of code. We provide the LLM with

an example by the developer that uses the same refactoring type on the commit the LLM will refactor. This

technique guides the model to follow the same approach as in the provided example. For instance, the prompt

would present a refactored code snippet where a method has been successfully extracted and then ask the

model to perform similar refactorings on a new code sample. This prompt aims to teach the model by example,

increasing the likelihood of correct and consistent refactoring operations.

Quality impact evaluation: To evaluate the impact of different prompting techniques, we compare the quality

of refactorings generated by StarCoder2 across the unit test pass rate which measures functional correctness and

compilability, the code smell reduction rate, and code quality metrics. The comparison allows us to assess whether

certain prompts improve the modelâ€™s performance on refactorings that developers typically perform better at. For

17

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Instruction
You are a powerful model specialized in refactoring Java code. Code refac-
toring is the process of improving the internal structure, readability, and
maintainability of a software codebase without altering its external behavior
or functionality. You must output a refactored version of the code. Explain
the steps you took to refactor the code and why you selected the refactoring
type/types you did.

Prompt
# Suggested refactoring types:
{refactorings developers performed on this commit with definitions}
# unrefactored code snippet(java):
{code_segment_before_refactoring}

Instruction
You are a powerful model specialized in refactoring Java code. Code refac-
toring is the process of improving the internal structure, readability, and
maintainability of a software codebase without altering its external behavior
or functionality. You must output a refactored version of the code.

Prompt
# unrefactored code snippet(java):
{code_segment_before_refactoring}
# refactored version of the same code snippet:
{developer_code_after_refactoring}

# unrefactored code snippet(java):
{code_segment_before_refactoring}

# refactored version of the same code snippet:

# refactored version of the same code snippet:

Fig. 7. Chain-of-Thought Prompt Used to Instruct Star-
Coder2 to Conduct Refactoring

Fig. 8. One-shot Prompt Used to Instruct StarCoder2 to
Conduct Refactoring

Table 7. Scott-Knott Test Comparing Unit Test Pass Rate and Code Smell Reduction Rates (SRR) across Different Prompting Techniques

Prompting Method

Unit Test Pass Rate (%)

SRR (%)

Scott-Knott Rank (SRR)

Zero-shot

Chain-of-thought

One-shot

28.36%

32.22%

34.51%

39.45%

42.34%

42.97%

2

1

1

each commit in our dataset, we generate refactorings using the 2 categories of prompts. We evaluate the effectiveness

of these prompts by comparing their ability to improve code quality metrics and reduce code smells, with particular

attention to whether StarCoder2 begins to perform refactorings it previously did not perform. To assess whether the

prompting techniques lead to statistically significant improvements, we perform the Scott-Knott test [11] across the

three distributions: zero-shot, one-shot, and chain-of-thought. The Scott-Knott [11] test is a hierarchical clustering

method that partitions distributions into statistically distinct groups, helping to identify whether the differences between

groups are meaningful [11]. The data samples consist of quality improvements (i.e., code smell reduction rate, and

percentage of improvement in code metrics) achieved by StarCoder2 under different prompt conditions for each project.

We compare the zero-shot prompt, one-shot prompt, and chain-of-thought prompt for their performance in each code

metric and code smell reduction to determine if the prompting techniques affect code refactoring performance and

compare the performance of three prompting techniques.

3.4.3 Findings. Applying one-shot prompting and chain-of-thought have more significant influences on

code smell reduction than applying zero-shot prompting. Table 7 summarizes the results for the unit test pass

rates and code smell reduction rates across the 3 prompting methods: zero-shot, one-shot, and chain-of-thought prompts.

Table 8 shows the results for code quality metric improvement rates across the 3 prompting methods.

â€¢ One-shot prompting: The one-shot prompt method achieves the highest performance, with a unit test pass rate
of 34.51% and a code smell reduction rate of 42.97% (Scott-Knott Rank 1). One-shot prompting is particularly

effective at reducing code complexity metrics such as AvgCyclomatic, Cyclomatic, and MaxCyclomatic, all of

which achieve the highest Scott-Knott rank (Rank 1), indicating consistent improvements. Additionally, this

method yields a significant reduction in CountClassCoupledModified (Rank 1), contributing to better modularity.
â€¢ Chain-of-thought prompting: This method also performs well, with a unit test pass rate of 32.22% and a
code smell reduction rate of 42.34% (Scott-Knott Rank 1). It demonstrates notable improvements in metrics

18

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 8. Scott-Knott Test Comparing Average Improvement in Each Code Metrics (%) across Different Prompting Methods

Metric

CountClassCoupled

CountClassCoupledModified

CountClassDerived

CountDeclClassVariable

CountDeclInstanceVariable

PercentLackOfCohesion

PercentLackOfCohesionModified

AvgCyclomatic

Cyclomatic

MaxCyclomatic

SumCyclomatic

Average

Zero-shot

Scott-Knott Rank (Zero-shot)

Chain-of-thought

Scott-Knott Rank (CoT)

One-shot

Scott-Knott Rank (One-shot)

21.4

18.3

17.9

12.5

19.7

22.8

23.9

17.4

16.5

15.8

18.6

19.32

2

3

2

2

2

2

2

2

2

2

2

2

23.4

19.2

18.1

14.0

20.2

23.5

24.1

18.0

17.0

16.3

19.1

19.99

1

2

2

1

2

1

2

1

1

1

1

1

22.7

20.1

18.3

13.8

19.8

23.1

24.2

18.4

17.3

16.6

19.3

20.15

1

1

2

1

2

2

2

1

1

1

1

1

like CountClassCoupled (Rank 1) and CountDeclClassVariable (Rank 1), with statistically significant differences.

Furthermore, chain-of-thought prompting increases the variety of refactoring types performed by the LLM,

adding seven new refactoring types not observed with zero-shot prompting: Extract Method, Rename Method,

Extract Variable, Inline Method, Add Parameter, Extract Class, and Parameterize Variable. This method enhances

the modelâ€™s ability to generate a broader range of refactorings by providing specific instructions and definitions.
â€¢ Zero-shot prompting: Exhibits the lowest performance, with a unit test pass rate of 28.36% and a code smell
reduction rate of 39.45% (Scott-Knott Rank 2). This suggests that StarCoder2â€™s ability to autonomously generate

high-quality refactorings is limited when given vague or minimal instructions. Metrics such as AvgCyclomatic, Cy-

clomatic, and MaxCyclomatic consistently receive a lower rank (Rank 2), indicating less significant improvements

compared to one-shot and chain-of-thought prompting.

One-shot prompting consistently yields the best performance across various metrics, including code complexity and

modularity, by providing a concrete example of refactoring. Chain-of-thought prompting follows closely, especially in

terms of expanding the types of refactorings performed and showing improvements in coupling and class variables. Both

methods significantly outperform the zero-shot baseline, reinforcing the importance of prompt design in optimizing the

performance of LLMs for refactoring tasks.

Summary of Results

Our findings highlight the importance of prompt design in leveraging the full potential of LLMs for code

refactoring tasks. Incorporating examples (i.e., one-shot prompt) or providing more explicit instructions (i.e.,

chain-of-thought prompt) significantly improves both the functional correctness and overall quality of the

refactorings generated by StarCoder2. The variation in code smell reduction rates and unit test pass rates across

different prompting methods underscores the value of prompt engineering in optimizing LLM performance for

complex tasks like refactoring.

4 THREATS TO VALIDITY

In this section, we outline potential threats to the validity of our study and the measures taken to mitigate them.

Threats to Internal Validity. Specify the validity of the methods applied in our study. While the performance of

StarCoder2 could theoretically be influenced by specific hardware and software configurations, we ensured that all

19

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

experiments were conducted under identical conditions across the same server, using consistent hardware and software

setups. Therefore, running the experiments on a different server should not significantly affect the results, as long as

comparable computational resources are used. This mitigates the concern of hardware or server variability as a threat

to internal validity.

A potential threat to internal validity is the occurrence of model hallucination, where StarCoder2 may generate

code that appears correct but is not logically or syntactically valid. Such hallucinations could alter the accuracy of our

results, particularly in evaluating unit test pass rates and refactoring effectiveness. We account for this by verifying

the output through unit test evaluations and inspecting for invalid code, but hallucinations remain an inherent risk in

LLM-generated code.

We utilize a randomly selected set of representative commits from our projects. However, our results may be

influenced by the random selection of commits, as variations in the project settings or the choice of different subsets

of commits could lead to differing outcomes. Some of the refactorings might span multiple commits, however, our

approach focuses on analyzing refactorings within a single commit. This limitation could result in missing refactorings

that are spread over several commits, potentially underestimating the complexity or scope of certain changes. Although

we select projects not included in the StarCoder2 training dataset, there is still a possibility that similar code patterns or

practices could be present in the training set of the model, which might give the LLM an advantage.

Threats to External Validity. Indicate the generalizability of our approach. Our experiments are conducted on a

limited set of open-source Java projects from the Apache dataset. Therefore, the findings may not be directly applicable

to projects in other programming languages or domains. We make the replication package publicly available to allow

other researchers to test across a more diverse set of projects and languages. Our study is based on the latest open-

source version of StarCoder2-15B-Instruct-v0.1, as of the date of our study. While our results reflect the capabilities of

this specific version, our evaluation approach is designed to be adaptable. It can be used to assess future versions of

StarCoder2 as well as other LLMs from different vendors. Nevertheless, different LLMs or different versions of StarCoder

may generate different results.

Threat to Construct Validity. Concern about our feature selection and our data preparation. We primarily rely on

code smell reduction, code quality metrics improvement, and unit test pass rates as indicators of refactoring quality.

However, these metrics may not fully capture all dimensions of code quality, such as readability and performance. This

limitation may affect the robustness of our conclusions in a broader range of quality indicators. We used Rminer3.0,

DesigniteJava, and Understand to gather our metrics for analysis which are the best in the state of practices for Java,

however different tools may get different results.

5 RELATED WORK

The application of Large Language Models (LLMs) in software engineering, particularly in code refactoring, has garnered

significant attention. In this section, we categorize the related work into three key areas: (1) code generation approaches

before and after the rise of LLMs, (2) code refactoring using LLMs, and (3) prompt engineering and fine-tuning techniques

to enhance LLM performance.

5.1 Code Generation Approaches: Pre-LLM and Post-LLM Era

Before the development of LLMs, code generation and repair primarily relied on rule-based systems and traditional

program synthesis techniques. These approaches typically use formal methods and predefined rules to generate code,

which limits their ability to handle complex programming tasks. For instance, DeepCoder [2] demonstrates an early

20

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

machine learning-based approach to code generation, synthesizing programs from input-output pairs, but struggling

with more complex scenarios. Similarly, AlphaCode [23] represents a more recent effort in competitive programming,

where LLMs like AlphaCode have shown remarkable improvements in generating functional code from natural language

descriptions.

With the rise of LLMs, a new standard for code generation has emerged. Studies, such as Codex [13], have demon-

strated the potential of LLMs to generate code through natural language prompts. Codex, for example, has been

evaluated for its ability to repair programs using test-based repair tools, showcasing strengths in code completion but

also highlighting limitations in handling complex tasks due to a lack of semantic understanding. Xu et al. [51] explore

the capabilities of LLMs in software engineering tasks, providing a systematic evaluation of models like Codex. Their

work emphasizes the strengths of LLMs in automating code generation and refactoring tasks, while also pointing out

the limitations of these models in understanding deeper program semantics.

Our work differs by focusing on the code refactoring capability of LLMs like StarCoder2, comparing their performance

with that of developers, specifically evaluating the reduction of code smells and improvements in code quality metrics,

which has been less explored in prior research.

5.2 Code Refactoring Using LLMs

LLMs have also been adopted for code refactoring tasks, where the focus is on improving code quality by enhancing

metrics like cyclomatic complexity and code modularity. Shirafuji et al. [42] demonstrate that LLM-generated refactorings

can lead to significant improvements in code metrics. By generating 10 candidate solutions (pass@10) with GPT-3.5,

they find 95.68% of programs could be successfully refactored, leading to a 17.35% reduction in cyclomatic complexity

and a 25.84% decrease in the average number of lines of code.

Our work builds on these studies by not only comparing LLM-generated refactorings with those performed by

developers but also by analyzing the effectiveness of different refactoring types. We evaluate how these refactorings

reduce code smells and improve key software metrics.

5.3 Prompt Engineering and Fine-Tuning for Improving LLMs

Prompt engineering has emerged as a technique for enhancing the performance of LLMs in various tasks, including code

refactoring. Studies have shown that by carefully designing input prompts, the accuracy and quality of LLM-generated

refactorings can be significantly improved. For instance, a notable study by Brown et al. [4] demonstrate that few-shot

prompting techniques can guide LLMs to generate higher-quality code, improving overall refactoring outcomes.

Large Language Models (LLMs) have also shown abilities in code generation through Chain-of-Thought (CoT)

prompting, which asks LLMs to first generate intermediate reasoning steps before outputting code. However, CoT

prompting has limitations in practical applications. For example, GPT-3.5-turbo with CoT prompting achieves only

53.29% Pass@1 in the HumanEval benchmark. Li et al. [21] propose a technique called Structured CoTs (SCoTs) to

overcome this limitation. SCoT prompting introduces programming structures (i.e., sequential, branch, and loop)

explicitly into the LLMâ€™s reasoning process, unlocking structured programming thinking in the model. Evaluations on

benchmarks such as HumanEval, MBPP, and MBCPP show that SCoT prompting outperforms CoT prompting by up

to 13.79% in Pass@1, and human developers prefer the programs generated by SCoT prompting over those from CoT

prompting.

Fine-tuning has also been explored as a method to improve the refactoring capabilities of pre-trained models. Recent

research [41] has shown that task-specific fine-tuning, combined with prompt design, can further enhance the ability

21

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

of LLMs to perform complex refactoring tasks. This approach leverages domain-specific data to refine the modelâ€™s

understanding of code structure and quality.

Our study builds on these findings by investigating how modifications to input prompts, including zero-shot, one-shot,

and chain-of-thought prompting, can improve the refactoring capabilities of StarCoder2. We aim to optimize prompt

designs to maximize the modelâ€™s effectiveness in reducing code smells and improving code metrics.

6 CONCLUSION

In this study we investigate the refactoring capabilities of StarCoder2, comparing its performance to human developers.

Our findings indicate that StarCoder2 outperforms developers in reducing implementation code smells, achieving a

reduction rate of 43.36%, compared to developersâ€™ code smell reduction rate of 24.27%. Moreover, StarCoder2 shows

better performance when performing systematic refactorings, while developers better handle more complex, context-

dependent design code smells. We observe that prompt engineering techniques, particularly one-shot prompting,

significantly improve refactoring quality, emphasizing the role of prompt design in maximizing the performance of

StarCoder2 in refactoring. Our results highlight the complementary strengths of LLMs, particularly StarCoder2, and

human expertise, suggesting best practices for utilizing LLMs in refactoring tasks. Future work will explore fine-tuning

strategies and assess the effectiveness of LLMs across more diverse projects and programming languages.

REFERENCES

[1] Nadia Alshahwan, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. 2024. Assured Offline LLM-Based Software
Engineering. In Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software
Engineering (Lisbon, Portugal) (InteNSE â€™24). Association for Computing Machinery, New York, NY, USA, 7â€“12. https://doi.org/10.1145/3643661.
3643953

[2] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. 2016. DeepCoder: Learning to Write Programs. CoRR

abs/1611.01989 (2016). arXiv:1611.01989 http://arxiv.org/abs/1611.01989

[3] Berkeley Bootcamp. 2020. 11 Most In-Demand Programming Languages in 2021. [Online]. Available: https://bootcamp.berkeley.edu/blog/most-

indemand-programming-languages/.

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural
Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877â€“1901.
https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[5] Jason Brownlee. 2020. Data leakage in machine learning. https://machinelearningmastery.com/data-leakage-machine-learning/
[6] Diego Cedrim, Alessandro Garcia, Melina Mongiovi, Rohit Gheyi, Leonardo Sousa, Rafael de Mello, Baldoino Fonseca, MÃ¡rcio Ribeiro, and Alexander
ChÃ¡vez. 2017. Understanding the impact of refactoring on smells: a longitudinal study of 23 software projects. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering (Paderborn, Germany) (ESEC/FSE 2017). Association for Computing Machinery, New York, NY,
USA, 465â€“475. https://doi.org/10.1145/3106237.3106259

[7] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A

survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15, 3 (2024), 1â€“45.

[8] Jinsu Choi, Gabin An, and Shin Yoo. 2024. Iterative Refactoring of Real-World Open-Source Programs with Large Language Models. In Proceedings
of the 16th International Symposium on Search-Based Software Engineering. Lecture Notes in Computer Science, Vol. 14767. Springer Nature,
49â€“55.

[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles
Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,
Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child,

22

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-
Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. arXiv:2204.02311 [cs.CL]
https://arxiv.org/abs/2204.02311

[10] M. Claes and M. V. MÃ¤ntylÃ¤. 2020. 20-mad: 20 years of issues and commits of mozilla and apache development. In Proceedings of the 17th

International Conference on Mining Software Repositories. 503â€“507.

[11] Enio G. Jelihovschi, Jose Claudio Faria, and Ivan Bezerra Allaman. 2014. ScottKnott: A Package for Performing the Scott-Knott Clustering Algorithm

in R. Trends in Applied and Computational Mathematics 15, 1 (2014), 3â€“17. https://tema.sbmac.org.br/tema/article/view/646/643

[12] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023. Large language models for software
engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering
(ICSE-FoSE). IEEE, 31â€“53.

[13] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated Repair of Programs from Large Language Models.
In Proceedings of the 45th International Conference on Software Engineering (Melbourne, Victoria, Australia) (ICSE â€™23). IEEE Press, 1469â€“1481.
https://doi.org/10.1109/ICSE48619.2023.00128

[14] Martin Fowler. 2018. Refactoring: improving the design of existing code. Addison-Wesley Professional.
[15] M. Fowler, K. Beck, J. Brant, W. Opdyke, and D. Roberts. 1999. Refactoring: Improving the Design of Existing Code. Addison-Wesley Professional,

Berkeley, CA, USA.

[16] Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic test suite generation for object-oriented software. SIGSOFT/FSE 2011 - Proceedings

of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering, 416â€“419. https://doi.org/10.1145/2025113.2025179

[17] Refactoring Guru. 2024. https://refactoring.guru/refactoring/techniques
[18] Avijit Hazra. 2017. Using the confidence interval confidently. Journal of thoracic disease 9, 10 (2017), 4125.
[19] Paul Jansen. 2024. https://www.tiobe.com/tiobe-index/
[20] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In
Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran
Associates, Inc., 22199â€“22213. https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf
[21] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering

and Methodology (2023).

[22] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny
Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoÃ£o Monteiro,
Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham
Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour
Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony
Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane
Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Sean
Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! arXiv:2305.06161 [cs.CL]
[23] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin

Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092â€“1097.

[24] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang

Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 (2024).

[25] Patrick E. McKnight and Julius Najab. 2010. Mann-Whitney U Test. John Wiley I& Sons, Ltd, 1â€“1. https://doi.org/10.1002/9780470479216.corpsy0524

arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470479216.corpsy0524

[26] Katherine Meissel and Ernest S. Yao. 2024. Using Cliffâ€™s Delta as a Non-Parametric Effect Size Measure: An Accessible Web App and R Tutorial.

Practical Assessment, Research, and Evaluation 29, 1 (2024), 2. https://doi.org/10.7275/pare.1977

[27] T. Mens and T. Tourwe. 2004. A survey of software refactoring.

IEEE Transactions on Software Engineering 30, 2 (2004), 126â€“139. https:

//doi.org/10.1109/TSE.2004.1265817

[28] Frederic P Miller, Agnes F Vandome, and John McBrewster. 2010. Apache Maven. Alpha Press.
[29] mockito. 2024. https://site.mockito.org/
[30] Shayan Noei, Heng Li, Stefanos Georgiou, and Ying Zou. 2023. An Empirical Study of Refactoring Rhythms and Tactics in the Software Development

Process. IEEE Transactions on Software Engineering 01 (2023), 1â€“17.

[31] Alberto S NuÃ±ez-Varela, HÃ©ctor G PÃ©rez-Gonzalez, Francisco E MartÃ­nez-Perez, and Carlos Soubervielle-Montalvo. 2017. Source code metrics: A

systematic mapping study. Journal of Systems and Software 128 (2017), 164â€“197.

[32] William Opdyke and Ralph Johnson. 1992. Refactoring Object-Oriented Frameworks. (07 1992).
[33] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff
Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa
Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson,
Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester

23

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Trovato et al.

Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
SimÃ³n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha
Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton,
Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino
Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan
Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Åukasz Kondraciuk,
Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim
Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,
Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,
Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David MÃ©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen Oâ€™Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,
Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,
Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri
Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam,
Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,
Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe CerÃ³n Uribe,
Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ
Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah
Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan
Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical
Report. arXiv:2303.08774 [cs.CL]

[34] Fabio Palomba, Gabriele Bavota, Massimiliano Di Penta, Fausto Fasano, Rocco Oliveto, and Andrea De Lucia. 2018. On the diffuseness and the impact
on maintainability of code smells: a large scale empirical investigation. In Proceedings of the 40th International Conference on Software Engineering
(Gothenburg, Sweden) (ICSE â€™18). Association for Computing Machinery, New York, NY, USA, 482. https://doi.org/10.1145/3180155.3182532
[35] Jevgenija Pantiuchina, Fiorella Zampetti, Simone Scalabrino, Valentina Piantadosi, Rocco Oliveto, Gabriele Bavota, and Massimiliano Di Penta.
2020. Why Developers Refactor Source Code: A Mining-based Study. ACM Trans. Softw. Eng. Methodol. 29, 4, Article 29 (sep 2020), 30 pages.
https://doi.org/10.1145/3408302

[36] Scientific Toolworks, Inc. 2023. Understand. https://scitools.com/static-code-analysis Software for Static Code Analysis.
[37] scitools. 2024. https://documentation.scitools.com/pdf/metricsdoc.pdf
[38] Tushar Sharma. 2024. https://tusharma.in/smells/CODE.html
[39] Tushar Sharma. 2024. Multi-faceted Code Smell Detection at Scale using DesigniteJava 2.0. In Proceedings of the 21st International Conference
on Mining Software Repositories (Lisbon, Portugal) (MSR â€™24). Association for Computing Machinery, New York, NY, USA, 284â€“288. https:
//doi.org/10.1145/3643991.3644881

[40] T. Sharma, P. Mishra, and R. Tiwari. 2016. Designite: A software design quality assessment tool. In Proceedings of the 1st International Workshop

on Bringing Architectural Design Thinking into Developersâ€™ Daily Activities. 1â€“4.

[41] Ensheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, and Hongbin Sun. 2023. Towards Efficient Fine-Tuning of Pre-trained
Code Models: An Experimental Study and Beyond. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and
Analysis (Seattle, WA, USA) (ISSTA 2023). Association for Computing Machinery, New York, NY, USA, 39â€“51. https://doi.org/10.1145/3597926.
3598036

[42] Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, and Yutaka Watanobe. 2023. Refactoring Programs Using Large Language Models with
Few-Shot Examples. In 2023 30th Asia-Pacific Software Engineering Conference (APSEC). 151â€“160. https://doi.org/10.1109/APSEC60848.2023.00025
[43] Danilo Silva, Nikolaos Tsantalis, and Marco Tulio Valente. 2016. Why we refactor? confessions of GitHub contributors. In Proceedings of the 2016
24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (Seattle, WA, USA) (FSE 2016). Association for Computing
Machinery, New York, NY, USA, 858â€“870. https://doi.org/10.1145/2950290.2950305
[44] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020.

Intellicode compose: Code generation using transformer.
In Proceedings of the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software
engineering. 1433â€“1443.

[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation
Language Models. arXiv:2302.13971 [cs.CL] https://arxiv.org/abs/2302.13971

[46] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig. 2020. RefactoringMiner 2.0. IEEE Transactions on Software Engineering 48, 3 (2020), 930â€“950.

24

An Empirical Study on the Code Refactoring Capability of Large Language Models

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

[47] Nikolaos Tsantalis, Matin Mansouri, Laleh M. Eshkevari, Davood Mazinanian, and Danny Dig. 2018. Accurate and Efficient Refactoring Detection in
Commit History. In Proceedings of the 40th International Conference on Software Engineering (Gothenburg, Sweden) (ICSE â€™18). ACM, New York,
NY, USA, 483â€“494. https://doi.org/10.1145/3180155.3180206

[48] Eva Van Emden and Leon Moonen. 2002. Java quality assurance by detecting code smells. In Ninth Working Conference on Reverse Engineering,

2002. Proceedings. IEEE, 97â€“106.

[49] Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. 2023. Copiloting the copilots: Fusing large language models with completion engines for
automated program repair. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. 172â€“184.

[50] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In

Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1â€“10.

[51] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In
Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming (San Diego, CA, USA) (MAPS 2022). Association for
Computing Machinery, New York, NY, USA, 1â€“10. https://doi.org/10.1145/3520312.3534862

[52] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-IDE Code Generation from Natural Language: Promise and Challenges. ACM Trans.

Softw. Eng. Methodol. 31, 2, Article 29 (mar 2022), 47 pages. https://doi.org/10.1145/3487569

[53] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Canâ€™t Prompt: How Non-AI Experts Try (and Fail)
to Design LLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (, Hamburg, Germany,) (CHI â€™23).
Association for Computing Machinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/3544548.3581388

[54] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie
Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (, Long Beach, CA, USA,) (KDD â€™23). Association for Computing Machinery,
New York, NY, USA, 5673â€“5684. https://doi.org/10.1145/3580305.3599790

25


